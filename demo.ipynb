{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from blocking_utils.blocking_utils import compute_similarity\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from nltk.util import ngrams\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "df = None\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "start_total = time.time()\n",
    "\n",
    "def get_all_pairs(cluster_dict):\n",
    "    pairs = set()\n",
    "    for records in cluster_dict.values():\n",
    "        if len(records) > 1:\n",
    "            pairs.update(combinations(sorted(records), 2))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def compute_similarity_pair(pair):\n",
    "    \"\"\"\n",
    "    Computes the similarity score for a given pair of record IDs.\n",
    "    Accesses the global DataFrame 'df'.\n",
    "    \"\"\"\n",
    "    record_id1, record_id2 = pair\n",
    "    # Access the global DataFrame\n",
    "    global df\n",
    "    try:\n",
    "        row1 = df.loc[record_id1]\n",
    "        row2 = df.loc[record_id2]\n",
    "        sim_score = compute_similarity(row1, row2)\n",
    "        return (pair, sim_score)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing similarity for pair {pair}: {e}\")\n",
    "        return (pair, 0.0)\n",
    "\n",
    "\n",
    "\n",
    "def create_ngram_lsh_parallel(\n",
    "    df_local, colname, n=3, threshold=0.5, num_perm=128, num_processes=4\n",
    "):\n",
    "    logger.info(f\"Creating parallel LSH for column: {colname}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize LSH\n",
    "    ngram_lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    ngram_minhashes = {}\n",
    "\n",
    "    # Split DataFrame into chunks\n",
    "    num_chunks = num_processes\n",
    "    chunks = np.array_split(df_local, num_chunks)\n",
    "\n",
    "    # Prepare arguments for each chunk\n",
    "    args = [(chunk, colname, n, num_perm) for chunk in chunks]\n",
    "\n",
    "    # Use Pool to compute MinHashes in parallel\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = list(tqdm(pool.imap(compute_minhash_chunk, args), total=len(args), desc=\"Processing Chunks\"))\n",
    "\n",
    "        results = pool.map(compute_minhash_chunk, args)\n",
    "    with ngram_lsh.insertion_session() as session:\n",
    "    # Collect MinHashes and insert into LSH\n",
    "        for partial_minhashes in results:\n",
    "            for record_id, m in partial_minhashes.items():\n",
    "                ngram_minhashes[record_id] = m\n",
    "                session.insert(record_id, m)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Parallel LSH creation for column {colname} completed in {time.time() - start_time:.2f} seconds.\"\n",
    "    )\n",
    "    return ngram_lsh, ngram_minhashes\n",
    "\n",
    "\n",
    "def compute_minhash_chunk(args):\n",
    "    \"\"\"\n",
    "    Computes MinHash signatures for a chunk of the DataFrame.\n",
    "    \"\"\"\n",
    "    chunk, colname, n, num_perm = args\n",
    "    from nltk.util import ngrams\n",
    "    from datasketch import MinHash\n",
    "    import pandas as pd\n",
    "\n",
    "    def text_to_ngrams(text, n):\n",
    "        text = text.lower().replace(\" \", \"\")\n",
    "        return set(\"\".join(ng) for ng in ngrams(text, n))\n",
    "\n",
    "    partial_minhashes = {}\n",
    "    for idx, row in tqdm(chunk.iterrows()):\n",
    "        col = row[colname]\n",
    "        record_id = row[\"record_id\"]\n",
    "\n",
    "        if pd.isnull(col):\n",
    "            continue\n",
    "\n",
    "        col_ngrams = text_to_ngrams(col, n)\n",
    "\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for ngram in col_ngrams:\n",
    "            m.update(ngram.encode(\"utf8\"))\n",
    "\n",
    "        partial_minhashes[record_id] = m\n",
    "    return partial_minhashes\n",
    "\n",
    "\n",
    "def create_minhash_text_based(text, n, num_perm=128):\n",
    "    text = text.lower().replace(\" \", \"\")\n",
    "    col_ngrams = set(\"\".join(ng) for ng in ngrams(text, n))\n",
    "    \n",
    "    # col_ngrams = text_to_ngrams(col, n)\n",
    "\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for ngram in col_ngrams:\n",
    "        m.update(ngram.encode(\"utf8\"))\n",
    "    return m\n",
    "\n",
    "\n",
    "def init_worker(dataframe):\n",
    "    \"\"\"\n",
    "    Initializer for worker processes to set the global DataFrame.\n",
    "    \"\"\"\n",
    "    global df\n",
    "    df = dataframe\n",
    "\n",
    "def process_record(record_id, col, lsh_dict, ngram, num_perm):\n",
    "    \"\"\"\n",
    "    Process a single record for LSH and returns composite key and record ID.\n",
    "    \"\"\"\n",
    "    global df\n",
    "    val = df[col].iloc[record_id]\n",
    "    if pd.isnull(val):\n",
    "        return None, None\n",
    "    try:\n",
    "        minhash = create_minhash_text_based(val, ngram, num_perm)\n",
    "        query = lsh_dict[col].query(minhash)\n",
    "        key = frozenset(query)\n",
    "        lsh_dict[col].insert(record_id, minhash)\n",
    "        return key, record_id\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing record {record_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "global df\n",
    "df = pd.read_csv(\"data/processed/external_parties_train.csv\")\n",
    "df[\"record_id\"] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "block diagram with overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code showing some examples of the preprocessed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing\n",
    "\n",
    "hash functions\n",
    "\n",
    "parameters\n",
    "\n",
    "elements in the same name bucket all have a very high similarity score\n",
    "    F1 score at .76 regardless of threshold\n",
    "elements in the same street name bucket have wide ranging similarity scores\n",
    "    F1 score at .55 with threshold 0.1, .83 with threshold .6, .81 with .7\n",
    "\n",
    "run-time: O(buckets * size_of_bucket^2)\n",
    "\n",
    "max block size\n",
    "- higher threshold\n",
    "- prune blocks too big\n",
    "\n",
    "Trade-off\n",
    "few and small buckets: FAST!, higher precision lower recall\n",
    "more buckets, bigger buckets: slooow, lower precision higher recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"parsed_name\", \"parsed_address_street_name\", \"parsed_address_city\"]\n",
    "thres = [0.3, 0.6, 0.6]\n",
    "ngram = [2, 3, 3]\n",
    "num_perm = 128\n",
    "\n",
    "lsh_dict = {\n",
    "    col: MinHashLSH(threshold=thres[i], num_perm=num_perm) \n",
    "    for i, col in enumerate(cols)\n",
    "}\n",
    "\n",
    "composite_key_to_records = defaultdict(set)\n",
    "futures = []\n",
    "for i, col in enumerate(cols):\n",
    "    for record_id in df[\"record_id\"]:\n",
    "        futures.append(\n",
    "            process_record(record_id, col, lsh_dict, ngram[i], num_perm)\n",
    "        )\n",
    "\n",
    "for future in futures:\n",
    "    key, record_id = future\n",
    "    if key is not None and record_id is not None:\n",
    "        composite_key_to_records[key].add(record_id)\n",
    "\n",
    "candidate_pairs = set()\n",
    "for records in composite_key_to_records.values():\n",
    "    if 100 > len(records) > 1:\n",
    "        candidate_pairs.update(combinations(sorted(records), 2))\n",
    "    elif len(records) > 100:\n",
    "        random_records = random.sample(sorted(records), 100)\n",
    "        candidate_pairs.update(combinations(random_records, 2))\n",
    "\n",
    "print(f\"Generated {len(candidate_pairs)} candidate pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Similarity\n",
    "\n",
    "threshold, no LLMs, strictness trade-off -> more precision less recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"def _compute_similarity(row1, row2):\n",
    "    # Initialize similarity score\n",
    "    similarity_score = 0.0\n",
    "    total_weight = 0.0\n",
    "\n",
    "    # Define weights for each feature\n",
    "    weights = {\n",
    "        \"is_company\": 2,\n",
    "        \"parsed_name\": 10.0,\n",
    "        \"name_phonetic\": 10.0,\n",
    "        \"surname\": 10.0,\n",
    "        \"surname_phonetic\": 5.0,\n",
    "        \"given_name\": 1.0,\n",
    "        \"surname_length\": 2,\n",
    "        \"party_iban\": 1.0,\n",
    "        \"party_phone\": 1.0,\n",
    "    }\n",
    "\n",
    "    # 1. Compare 'is_company'\n",
    "    if not pd.isnull(row1[\"is_company\"]) and not pd.isnull(row2[\"is_company\"]):\n",
    "        if row1[\"is_company\"] == row2[\"is_company\"]:\n",
    "            similarity_score += weights[\"is_company\"]\n",
    "        total_weight += weights[\"is_company\"]\n",
    "\n",
    "    # 2. Compare 'parsed_name' using Jaro-Winkler similarity\n",
    "    if not pd.isnull(row1[\"parsed_name\"]) and not pd.isnull(row2[\"parsed_name\"]):\n",
    "        name_similarity = jellyfish.jaro_winkler_similarity(\n",
    "            row1[\"parsed_name\"], row2[\"parsed_name\"]\n",
    "        )\n",
    "        similarity_score += name_similarity * weights[\"parsed_name\"]\n",
    "        total_weight += weights[\"parsed_name\"]\n",
    "\n",
    "    # 3. Compare name phonetic encodings\n",
    "    phonetic_matches = 0\n",
    "    phonetic_total = 0\n",
    "\n",
    "    for encoding in [\"name_soundex\", \"name_metaphone\", \"name_nysiis\"]:\n",
    "        if encoding in row1 and encoding in row2:\n",
    "            if not pd.isnull(row1[encoding]) and not pd.isnull(row2[encoding]):\n",
    "                phonetic_total += 1\n",
    "                if row1[encoding] == row2[encoding]:\n",
    "                    phonetic_matches += 1\n",
    "\n",
    "    if phonetic_total > 0:\n",
    "        phonetic_similarity = phonetic_matches / phonetic_total\n",
    "        similarity_score += phonetic_similarity * weights[\"name_phonetic\"]\n",
    "        total_weight += weights[\"name_phonetic\"]\n",
    "\n",
    "    # 4. Compare 'surname' using Jaro-Winkler similarity\n",
    "    if not pd.isnull(row1[\"surname\"]) and not pd.isnull(row2[\"surname\"]):\n",
    "        surname_similarity = jellyfish.jaro_winkler_similarity(\n",
    "            row1[\"surname\"], row2[\"surname\"]\n",
    "        )\n",
    "        similarity_score += surname_similarity * weights[\"surname\"]\n",
    "        total_weight += weights[\"surname\"]\n",
    "\n",
    "    # 5. Compare surname phonetic encodings\n",
    "    surname_phonetic_matches = 0\n",
    "    surname_phonetic_total = 0\n",
    "\n",
    "    for encoding in [\"surname_soundex\", \"surname_metaphone\", \"surname_nysiis\"]:\n",
    "        if encoding in row1 and encoding in row2:\n",
    "            if not pd.isnull(row1[encoding]) and not pd.isnull(row2[encoding]):\n",
    "                surname_phonetic_total += 1\n",
    "                if row1[encoding] == row2[encoding]:\n",
    "                    surname_phonetic_matches += 1\n",
    "\n",
    "    if surname_phonetic_total > 0:\n",
    "        surname_phonetic_similarity = surname_phonetic_matches / surname_phonetic_total\n",
    "        similarity_score += surname_phonetic_similarity * weights[\"surname_phonetic\"]\n",
    "        total_weight += weights[\"surname_phonetic\"]\n",
    "\n",
    "    # 6. Compare 'given_name' using Jaro-Winkler similarity\n",
    "    if not pd.isnull(row1[\"given_name\"]) and not pd.isnull(row2[\"given_name\"]):\n",
    "        given_name_similarity = jellyfish.jaro_winkler_similarity(\n",
    "            row1[\"given_name\"], row2[\"given_name\"]\n",
    "        )\n",
    "        similarity_score += given_name_similarity * weights[\"given_name\"]\n",
    "        total_weight += weights[\"given_name\"]\n",
    "\n",
    "    # 7. Compare 'surname_length'\n",
    "    if not pd.isnull(row1[\"surname_length\"]) and not pd.isnull(row2[\"surname_length\"]):\n",
    "        length_difference = abs(row1[\"surname_length\"] - row2[\"surname_length\"])\n",
    "        max_length = max(row1[\"surname_length\"], row2[\"surname_length\"])\n",
    "        if max_length > 0:\n",
    "            length_similarity = 1 - (length_difference / max_length)\n",
    "            similarity_score += length_similarity * weights[\"surname_length\"]\n",
    "            total_weight += weights[\"surname_length\"]\n",
    "\n",
    "    # 8. Compare 'party_iban' if available\n",
    "    if \"party_iban\" in row1 and \"party_iban\" in row2:\n",
    "        if not pd.isnull(row1[\"party_iban\"]) and not pd.isnull(row2[\"party_iban\"]):\n",
    "            if row1[\"party_iban\"] == row2[\"party_iban\"]:\n",
    "                similarity_score += weights[\"party_iban\"]\n",
    "            total_weight += weights[\"party_iban\"]\n",
    "\n",
    "    # 9. Compare 'party_phone' if available\n",
    "    if \"party_phone\" in row1 and \"party_phone\" in row2:\n",
    "        if not pd.isnull(row1[\"party_phone\"]) and not pd.isnull(row2[\"party_phone\"]):\n",
    "            phone_similarity = jellyfish.jaro_winkler_similarity(\n",
    "                row1[\"party_phone\"], row2[\"party_phone\"]\n",
    "            )\n",
    "            similarity_score += phone_similarity * weights[\"party_phone\"]\n",
    "            total_weight += weights[\"party_phone\"]\n",
    "\n",
    "    # Handle case where total_weight is zero to avoid division by zero\n",
    "    if total_weight == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate final similarity score as a percentage\n",
    "    final_similarity = (similarity_score / total_weight)\n",
    "\n",
    "    return final_similarity\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.5\n",
    "matched_pairs = set()\n",
    "unmatched_candidate_pairs = []\n",
    "\n",
    "start_similarity = time.time()\n",
    "\n",
    "for pair in candidate_pairs:\n",
    "    first = df.loc[df[\"record_id\"] == pair[0]].iloc[0]\n",
    "    second = df.loc[df[\"record_id\"] == pair[1]].iloc[0]\n",
    "    sim_score = compute_similarity(first, second)\n",
    "    if sim_score >= similarity_threshold:\n",
    "        matched_pairs.add(pair)\n",
    "    else:\n",
    "        unmatched_candidate_pairs.append(pair)\n",
    "\n",
    "print(\n",
    "    f\"Similarity computation completed in {time.time() - start_similarity:.2f} seconds.\"\n",
    ")\n",
    "print(f\"Matched pairs after similarity threshold: {len(matched_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matched pairs after adding IBAN and phone: 6063\n"
     ]
    }
   ],
   "source": [
    "# Additional pairing based on 'party_iban' and 'party_phone'\n",
    "party_iban_to_record_ids = (\n",
    "    df.groupby(\"party_iban\")[\"record_id\"].apply(list).to_dict()\n",
    ")\n",
    "party_phone_to_record_ids = (\n",
    "    df.groupby(\"party_phone\")[\"record_id\"].apply(list).to_dict()\n",
    ")\n",
    "\n",
    "for record_ids in party_iban_to_record_ids.values():\n",
    "    if len(record_ids) > 1:\n",
    "        matched_pairs.update(combinations(sorted(record_ids), 2))\n",
    "for record_ids in party_phone_to_record_ids.values():\n",
    "    if len(record_ids) > 1:\n",
    "        matched_pairs.update(combinations(sorted(record_ids), 2))\n",
    "\n",
    "print(\n",
    "    f\"Total matched pairs after adding IBAN and phone: {len(matched_pairs)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union-Find implementation\n",
    "start_union_find = time.time()\n",
    "parent = {record_id: record_id for record_id in df[\"record_id\"]}\n",
    "\n",
    "def find(u):\n",
    "    while parent[u] != u:\n",
    "        parent[u] = parent[parent[u]]\n",
    "        u = parent[u]\n",
    "    return u\n",
    "\n",
    "def union(u, v):\n",
    "    pu, pv = find(u), find(v)\n",
    "    if pu != pv:\n",
    "        parent[pu] = pv\n",
    "\n",
    "for u, v in matched_pairs:\n",
    "    union(u, v)\n",
    "\n",
    "# Generate clusters\n",
    "clusters = defaultdict(set)\n",
    "for record_id in df[\"record_id\"]:\n",
    "    cluster_id = find(record_id)\n",
    "    clusters[cluster_id].add(record_id)\n",
    "\n",
    "# Generate predictions\n",
    "# df[\"predicted_external_id\"] = df[\"record_id\"].apply(lambda x: find(x))\n",
    "\n",
    "print(\n",
    "    f\"Clustering completed in {time.time() - start_union_find:.2f} seconds.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Evaluation\n",
    "\n",
    "We first **aggregate pair-matchings with Union-Find**\n",
    "\n",
    "We evaluated our models on the training data. We do so by by looking at each pair of elements and label our prediction as a: \n",
    "* **True Positive**: $i$ and $j$ are the same entity and we predicted so.\n",
    "* **False Positive**: we predicted $i$ and $j$ as the same entity, which is false.\n",
    "* **False Negative**: we did not identify $i$ and $j$ as the same entity.\n",
    "\n",
    "The evaluation metrics we used to guide our modeling are **Precision** and **Recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9786\n",
      "Recall: 0.4888\n",
      "F1-Score: 0.6519\n"
     ]
    }
   ],
   "source": [
    "# Ground truth clusters based on 'external_id'\n",
    "ground_truth = df.groupby(\"external_id\")[\"record_id\"].apply(set).to_dict()\n",
    "\n",
    "# Get true pairs and predicted pairs\n",
    "true_pairs = get_all_pairs(ground_truth)\n",
    "predicted_pairs = get_all_pairs(clusters)\n",
    "\n",
    "# Compute True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "TP = len(true_pairs.intersection(predicted_pairs))\n",
    "FP = len(predicted_pairs.difference(true_pairs))\n",
    "FN = len(true_pairs.difference(predicted_pairs))\n",
    "\n",
    "# Precision, Recall, F1-Score\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_score = (\n",
    "    2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    ")\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
